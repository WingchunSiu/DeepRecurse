<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>RLM Multi-Agent Platform â€” Architecture Plan</title>
<style>
  :root {
    --bg: #0d1117;
    --surface: #161b22;
    --surface2: #1c2333;
    --surface3: #21262d;
    --border: #30363d;
    --text: #e6edf3;
    --text-muted: #8b949e;
    --accent: #58a6ff;
    --accent2: #bc8cff;
    --green: #3fb950;
    --orange: #d29922;
    --red: #f85149;
    --pink: #f778ba;
    --cyan: #79c0ff;
  }
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
    background: var(--bg);
    color: var(--text);
    line-height: 1.65;
    padding: 2rem;
    max-width: 1100px;
    margin: 0 auto;
  }
  h1 {
    font-size: 2rem;
    margin-bottom: 0.3rem;
    background: linear-gradient(135deg, var(--accent), var(--accent2));
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
  }
  .subtitle { color: var(--text-muted); margin-bottom: 2rem; font-size: 1rem; }
  h2 {
    font-size: 1.4rem;
    margin: 2.5rem 0 1rem;
    padding-bottom: 0.4rem;
    border-bottom: 2px solid var(--border);
    color: var(--accent);
  }
  h3 { font-size: 1.1rem; margin: 1.5rem 0 0.6rem; color: var(--accent2); }
  h4 { font-size: 0.95rem; margin: 1rem 0 0.4rem; color: var(--cyan); }
  p { margin-bottom: 0.75rem; }
  ul, ol { margin: 0.5rem 0 1rem 1.5rem; }
  li { margin-bottom: 0.3rem; }

  pre {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 8px;
    padding: 1rem 1.2rem;
    overflow-x: auto;
    font-size: 0.85rem;
    line-height: 1.5;
    margin: 0.75rem 0;
    font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
  }
  code {
    font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
    font-size: 0.88em;
  }
  :not(pre) > code {
    background: var(--surface2);
    padding: 0.15rem 0.4rem;
    border-radius: 4px;
    font-size: 0.85em;
  }

  .callout {
    border-left: 4px solid var(--accent);
    background: rgba(88,166,255,0.06);
    padding: 1rem 1.2rem;
    border-radius: 0 8px 8px 0;
    margin: 1rem 0;
  }
  .callout-green { border-left-color: var(--green); background: rgba(63,185,80,0.06); }
  .callout-orange { border-left-color: var(--orange); background: rgba(210,153,34,0.06); }
  .callout-pink { border-left-color: var(--pink); background: rgba(247,120,186,0.06); }
  .callout strong { color: var(--accent); }
  .callout-green strong { color: var(--green); }
  .callout-orange strong { color: var(--orange); }
  .callout-pink strong { color: var(--pink); }

  .tag {
    display: inline-block;
    padding: 0.12rem 0.45rem;
    border-radius: 5px;
    font-size: 0.75rem;
    font-weight: 600;
    margin: 0.1rem;
  }
  .tag-green { background: rgba(63,185,80,0.15); color: var(--green); }
  .tag-blue { background: rgba(88,166,255,0.15); color: var(--accent); }
  .tag-purple { background: rgba(188,140,255,0.15); color: var(--accent2); }
  .tag-orange { background: rgba(210,153,34,0.15); color: var(--orange); }
  .tag-pink { background: rgba(247,120,186,0.15); color: var(--pink); }
  .tag-cyan { background: rgba(121,192,255,0.15); color: var(--cyan); }

  table {
    width: 100%;
    border-collapse: collapse;
    margin: 0.75rem 0 1.5rem;
    font-size: 0.9rem;
  }
  th, td {
    padding: 0.6rem 0.8rem;
    border: 1px solid var(--border);
    text-align: left;
    vertical-align: top;
  }
  th { background: var(--surface2); color: var(--accent); font-weight: 600; }

  .flow-diagram {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 10px;
    padding: 1.5rem;
    font-family: 'SFMono-Regular', Consolas, monospace;
    font-size: 0.82rem;
    line-height: 1.5;
    overflow-x: auto;
    white-space: pre;
    color: var(--text-muted);
    margin: 1rem 0;
  }

  .step-card {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 10px;
    padding: 1.2rem;
    margin: 1rem 0;
  }
  .step-card h4 {
    margin: 0 0 0.5rem;
    display: flex;
    align-items: center;
    gap: 0.5rem;
  }
  .step-num {
    background: var(--accent);
    color: var(--bg);
    width: 24px;
    height: 24px;
    border-radius: 50%;
    display: inline-flex;
    align-items: center;
    justify-content: center;
    font-size: 0.75rem;
    font-weight: 700;
    flex-shrink: 0;
  }

  .grid-2 { display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1rem 0; }
  .grid-3 { display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 1rem; margin: 1rem 0; }
  @media (max-width: 800px) { .grid-2, .grid-3 { grid-template-columns: 1fr; } }

  .scenario-card {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 10px;
    padding: 1rem;
  }
  .scenario-card h4 { margin-top: 0; }

  .toc {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 12px;
    padding: 1.5rem 2rem;
    margin-bottom: 2rem;
  }
  .toc h2 { font-size: 1rem; margin: 0 0 0.75rem; border: none; padding: 0; }
  .toc ol { margin: 0; padding-left: 1.5rem; }
  .toc li { margin-bottom: 0.25rem; }
  .toc a { color: var(--accent); text-decoration: none; }
  .toc a:hover { text-decoration: underline; }

  footer {
    text-align: center;
    padding: 2rem 0;
    color: var(--text-muted);
    border-top: 1px solid var(--border);
    margin-top: 3rem;
    font-size: 0.85rem;
  }
</style>
</head>
<body>

<h1>RLM Multi-Agent Platform</h1>
<p class="subtitle">Architecture plan for a parallel multi-agent system with interactive web UI, built on rlm-minimal</p>

<nav class="toc">
  <h2>Contents</h2>
  <ol>
    <li><a href="#vision">The Vision</a></li>
    <li><a href="#insight">Key Architectural Insight</a></li>
    <li><a href="#architecture">System Architecture</a></li>
    <li><a href="#agent-types">Agent Types &amp; Use Cases</a></li>
    <li><a href="#components">Component Design</a></li>
    <li><a href="#data-flow">Data Flow &amp; Events</a></li>
    <li><a href="#file-structure">File Structure</a></li>
    <li><a href="#implementation">Implementation Steps</a></li>
    <li><a href="#ui-design">UI Design</a></li>
    <li><a href="#tools">Tool System</a></li>
    <li><a href="#concurrency">Concurrency &amp; Safety</a></li>
    <li><a href="#decisions">Design Decisions &amp; Trade-offs</a></li>
    <li><a href="#future">Future Extensions</a></li>
  </ol>
</nav>

<!-- ================================================================ -->
<section id="vision">
<h2>1. The Vision</h2>
<p>A platform where you launch <strong>multiple RLM-powered agents in parallel</strong>, each tackling a different task. Some work recursively on long documents. Others chain tools together. A web dashboard lets you watch them all work in real time &mdash; seeing their code, output, and reasoning as it happens.</p>

<div class="grid-3">
  <div class="scenario-card">
    <h4 style="color: var(--green);">Agent A: Legal Doc Analyst</h4>
    <p><span class="tag tag-green">recursive_rlm</span></p>
    <p>Processes a 200-page legal document. Chunks it, queries sub-LLMs on each section, aggregates findings. 10-15 iterations.</p>
  </div>
  <div class="scenario-card">
    <h4 style="color: var(--pink);">Agent B: Image Pipeline</h4>
    <p><span class="tag tag-pink">tool_chain</span></p>
    <p>Generates an AI image, converts to 3D, outputs a printable figure. Calls tools in sequence. 2-3 iterations.</p>
  </div>
  <div class="scenario-card">
    <h4 style="color: var(--accent2);">Agent C: Dataset Curator</h4>
    <p><span class="tag tag-purple">hybrid</span></p>
    <p>Cleans a dirty dataset using code + sub-LLM calls for semantic fixes, then generates synthetic rows. 5-10 iterations.</p>
  </div>
</div>
</section>

<!-- ================================================================ -->
<section id="insight">
<h2>2. Key Architectural Insight</h2>

<div class="callout-green callout">
  <strong>The RLM REPL is already a general-purpose execution environment.</strong>
  <p style="margin-top: 0.5rem;">The <code>REPLEnv</code> class (in <code>rlm/repl.py</code>) has a <code>self.globals</code> dict where Python functions are injected. Currently it has <code>llm_query()</code> and <code>FINAL_VAR()</code>. <strong>Tools are just more functions injected into that same dict.</strong></p>
  <p>A "non-recursive agent" is simply an RLM with <code>max_iterations=3</code> and tool functions available. A "hybrid agent" has both <code>llm_query()</code> AND tools. No new execution model is needed.</p>
</div>

<div class="callout">
  <strong>Each <code>REPLEnv</code> instance is already isolated</strong> &mdash; own globals dict, own locals dict, own temp directory, own thread lock. Multiple agents running in parallel each get their own <code>REPLEnv</code> with zero shared state. This is exactly the isolation model we need for a multi-agent system.
</div>
</section>

<!-- ================================================================ -->
<section id="architecture">
<h2>3. System Architecture</h2>

<div class="flow-diagram"><span style="color: var(--accent);">Browser (ui.html)</span>
    |                           ^
    | POST /api/agents          | SSE events (real-time)
    | (launch, cancel)          | GET /api/events
    v                           |
<span style="color: var(--green);">FastAPI Server (server.py)</span>
    |
    v
<span style="color: var(--accent2);">Orchestrator</span>
    |--- EventBus (thread-safe pub/sub, history replay)
    |--- ToolRegistry (named functions)
    |--- ThreadPoolExecutor (max_workers=8)
    |
    +--- <span style="color: var(--orange);">Thread 1: Agent A</span> (recursive_rlm)
    |       |--- OpenAIClient (gpt-4o)
    |       |--- REPLEnv (context + llm_query)
    |       |--- Iteration loop â†’ emits events
    |
    +--- <span style="color: var(--pink);">Thread 2: Agent B</span> (tool_chain)
    |       |--- OpenAIClient (gpt-4o-mini)
    |       |--- REPLEnv (tools: generate_image, make_3d, ...)
    |       |--- 2-3 iterations â†’ emits events
    |
    +--- <span style="color: var(--accent2);">Thread 3: Agent C</span> (hybrid)
            |--- OpenAIClient (gpt-4o)
            |--- REPLEnv (llm_query + tools: save_json, augment_dataset, ...)
            |--- 5-10 iterations â†’ emits events</div>
</section>

<!-- ================================================================ -->
<section id="agent-types">
<h2>4. Agent Types &amp; Use Cases</h2>

<table>
  <thead>
    <tr><th>Type</th><th>Max Iterations</th><th>Has llm_query()</th><th>Has Tools</th><th>Use Cases</th></tr>
  </thead>
  <tbody>
    <tr>
      <td><span class="tag tag-green">recursive_rlm</span></td>
      <td>10-30</td>
      <td>Yes</td>
      <td>Optional</td>
      <td>Long document analysis, research, needle-in-haystack, multi-step reasoning</td>
    </tr>
    <tr>
      <td><span class="tag tag-pink">tool_chain</span></td>
      <td>1-3</td>
      <td>Optional</td>
      <td>Yes</td>
      <td>Image generation pipelines, file processing, API calls, 3D conversion</td>
    </tr>
    <tr>
      <td><span class="tag tag-purple">hybrid</span></td>
      <td>5-10</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>Dataset curation (code + LLM semantics), research + action combos</td>
    </tr>
  </tbody>
</table>

<p>The difference is purely in <strong>what functions are available in the REPL namespace</strong> and <strong>how many iterations the agent gets</strong>. The underlying loop is identical for all three types.</p>
</section>

<!-- ================================================================ -->
<section id="components">
<h2>5. Component Design</h2>

<h3>5.1 AgentConfig &amp; AgentState</h3>
<pre><code># agents/types.py

@dataclass
class AgentConfig:
    name: str                          # Display name ("Legal Analyst")
    task: str                          # The prompt/query
    agent_type: AgentType              # recursive_rlm | tool_chain | hybrid
    context: str | dict | list | None  # Optional context data
    model: str = "gpt-4o-mini"         # Root LLM model
    recursive_model: str = "gpt-4o-mini"  # Sub-LLM model
    max_iterations: int = 20           # Auto-adjusted by type
    tools: list[str] = []              # Tool names from registry

@dataclass
class AgentState:
    id: str                     # UUID (short, e.g. "a3f2b1c8")
    config: AgentConfig
    status: AgentStatus         # pending â†’ running â†’ completed/failed/cancelled
    current_iteration: int = 0
    max_iterations: int = 20
    started_at: float | None
    completed_at: float | None
    last_code: str | None       # Most recent code block executed
    last_stdout: str | None     # Most recent stdout
    last_stderr: str | None     # Most recent stderr
    final_answer: str | None
    error: str | None
    elapsed_seconds: float = 0.0</code></pre>

<h3>5.2 AgentEvent</h3>
<pre><code># Emitted by agents, consumed by SSE endpoint

class EventType(Enum):
    AGENT_CREATED       # Agent config registered
    AGENT_STARTED       # Thread started executing
    ITERATION_STARTED   # Beginning iteration N of M
    CODE_EXECUTED       # Code block ran, has stdout/stderr
    FINAL_ANSWER        # Agent produced final result
    AGENT_COMPLETED     # Clean finish
    AGENT_FAILED        # Exception occurred

@dataclass
class AgentEvent:
    event_type: EventType
    agent_id: str
    timestamp: float
    data: dict          # Event-specific payload

    def to_sse(self) -> str:
        """Format as Server-Sent Event for streaming."""
        return f"event: {self.event_type.value}\ndata: {json.dumps(payload)}\n\n"</code></pre>

<h3>5.3 EventBus</h3>
<pre><code># agents/events.py â€” Thread-safe publish/subscribe

class EventBus:
    def emit(event: AgentEvent)
        # Called by agent threads. Pushes to all subscriber queues.

    def subscribe(include_history=True) -> Generator[AgentEvent]
        # Called by SSE endpoint. Yields events as they arrive.
        # include_history=True â†’ replays all past events first,
        #   so late-joining UI clients see current state.
        # Sends heartbeat (None) every 30s to keep connection alive.</code></pre>

<div class="callout">
  <strong>Why per-subscriber queues?</strong> Multiple browser tabs can connect to SSE simultaneously. Each gets its own <code>queue.Queue</code>. When an agent emits an event, it's pushed to all queues. This is simpler and more correct than a shared log with polling.
</div>

<h3>5.4 Agent (the core class)</h3>
<p>The Agent <strong>re-implements the iteration loop from <code>rlm_repl.py</code></strong> (lines 84-121) but calls the exact same utility functions. This lets us emit events between each step without modifying the <code>rlm/</code> package.</p>

<pre><code># agents/agent.py â€” simplified pseudocode

class Agent:
    def __init__(self, config, event_bus, tool_registry):
        self.state = AgentState(config=config)
        # ...

    def run(self):  # Called in worker thread
        emit(AGENT_STARTED)

        llm = OpenAIClient(model=config.model)
        messages = build_system_prompt()          # from rlm.utils.prompts
        # Append tool descriptions to system prompt
        repl_env = REPLEnv(context, recursive_model)
        # Inject tools into repl_env.globals

        for iteration in range(max_iterations):
            if self.state.status == CANCELLED: return

            emit(ITERATION_STARTED, {iteration, max_iterations})

            response = llm.completion(messages + [next_action_prompt(...)])

            code_blocks = find_code_blocks(response)  # rlm.utils.utils
            for code in code_blocks:
                result = repl_env.code_execution(code)  # rlm.repl
                emit(CODE_EXECUTED, {code, stdout, stderr, exec_time})
                messages = add_execution_result_to_messages(...)

            final = check_for_final_answer(response, repl_env, ...)
            if final:
                emit(FINAL_ANSWER, {answer})
                emit(AGENT_COMPLETED, {elapsed})
                return

        # Exhausted iterations â†’ force final answer
        emit(AGENT_COMPLETED, ...)</code></pre>

<div class="callout-orange callout">
  <strong>Why not just call <code>RLM_REPL.completion()</code>?</strong> Because it's a black box &mdash; we can't emit events mid-iteration. By replaying the same ~40 lines of logic using the same utility functions (<code>find_code_blocks</code>, <code>check_for_final_answer</code>, <code>add_execution_result_to_messages</code>, etc.), we get identical behavior with full observability. Zero changes to <code>rlm/</code>.
</div>

<h3>5.5 Orchestrator</h3>
<pre><code># agents/orchestrator.py

class Orchestrator:
    event_bus: EventBus
    tool_registry: ToolRegistry
    executor: ThreadPoolExecutor(max_workers=8)
    agents: dict[str, Agent]

    def launch_agent(config: AgentConfig) -> AgentState
        # Create Agent, submit agent.run to thread pool

    def get_all_states() -> list[dict]
        # Current state of every agent (for API response)

    def cancel_agent(agent_id) -> bool
        # Set agent status to cancelled (cooperative)</code></pre>

<h3>5.6 FastAPI Server</h3>
<table>
  <thead>
    <tr><th>Endpoint</th><th>Method</th><th>Purpose</th></tr>
  </thead>
  <tbody>
    <tr><td><code>POST /api/agents</code></td><td>POST</td><td>Launch a new agent with config (name, task, type, context, tools)</td></tr>
    <tr><td><code>GET /api/agents</code></td><td>GET</td><td>List all agents + their current states</td></tr>
    <tr><td><code>GET /api/agents/{id}</code></td><td>GET</td><td>Get single agent's full state</td></tr>
    <tr><td><code>POST /api/agents/{id}/cancel</code></td><td>POST</td><td>Cancel a running agent</td></tr>
    <tr><td><code>GET /api/tools</code></td><td>GET</td><td>List available tools + descriptions</td></tr>
    <tr><td><code>GET /api/events</code></td><td>GET</td><td>SSE stream â€” real-time events from all agents</td></tr>
    <tr><td><code>GET /</code></td><td>GET</td><td>Serve the UI (ui.html)</td></tr>
  </tbody>
</table>

<p>The SSE endpoint bridges the blocking <code>EventBus.subscribe()</code> generator to an async <code>StreamingResponse</code> using a reader thread + <code>asyncio.Queue</code>.</p>
</section>

<!-- ================================================================ -->
<section id="data-flow">
<h2>6. Data Flow &amp; Events</h2>

<h3>Complete lifecycle of one agent</h3>
<div class="flow-diagram">User clicks "Launch Agent" in browser
    |
    |  POST /api/agents {name: "Legal Analyst", task: "Summarize...", type: "recursive_rlm"}
    v
<span style="color: var(--green);">server.py</span> â†’ orchestrator.launch_agent(config)
    |
    v
<span style="color: var(--accent2);">orchestrator</span> â†’ creates Agent â†’ submits agent.run() to ThreadPoolExecutor
    |
    v
<span style="color: var(--orange);">agent thread starts</span>
    |
    +--â†’ emit(<span style="color: var(--cyan);">AGENT_CREATED</span>)     â†’ EventBus â†’ SSE â†’ Browser creates card
    +--â†’ emit(<span style="color: var(--cyan);">AGENT_STARTED</span>)     â†’ card shows "running" badge
    |
    +--â†’ <span style="color: var(--text);">Iteration 1:</span>
    |     +--â†’ emit(<span style="color: var(--cyan);">ITERATION_STARTED</span>)  â†’ progress bar: 1/20
    |     +--â†’ LLM call â†’ response with ```repl``` code
    |     +--â†’ exec(code) in REPLEnv
    |     +--â†’ emit(<span style="color: var(--cyan);">CODE_EXECUTED</span>)       â†’ card shows code + stdout
    |
    +--â†’ <span style="color: var(--text);">Iteration 2-N:</span>  (same pattern)
    |     +--â†’ progress bar advances, code/stdout updates
    |
    +--â†’ <span style="color: var(--text);">Final iteration:</span>
    |     +--â†’ LLM returns FINAL(answer)
    |     +--â†’ emit(<span style="color: var(--cyan);">FINAL_ANSWER</span>)       â†’ card shows answer
    |     +--â†’ emit(<span style="color: var(--cyan);">AGENT_COMPLETED</span>)    â†’ card shows "completed" + elapsed time
    v
<span style="color: var(--green);">Done.</span> Agent thread exits, state persists in orchestrator.</div>

<h3>Event Payloads</h3>
<table>
  <thead>
    <tr><th>Event</th><th>Key Fields</th></tr>
  </thead>
  <tbody>
    <tr><td><code>agent_created</code></td><td>name, task, agent_type, max_iterations, tools</td></tr>
    <tr><td><code>agent_started</code></td><td>started_at</td></tr>
    <tr><td><code>iteration_started</code></td><td>iteration (1-indexed), max_iterations</td></tr>
    <tr><td><code>code_executed</code></td><td>iteration, code (truncated 2K), stdout (2K), stderr (500), execution_time</td></tr>
    <tr><td><code>final_answer</code></td><td>answer (truncated 5K), iterations_used</td></tr>
    <tr><td><code>agent_completed</code></td><td>elapsed_seconds</td></tr>
    <tr><td><code>agent_failed</code></td><td>error message</td></tr>
  </tbody>
</table>
</section>

<!-- ================================================================ -->
<section id="file-structure">
<h2>7. File Structure</h2>

<pre><code>rlm-minimal/
  rlm/                          # EXISTING â€” <strong>untouched</strong>
    __init__.py
    rlm.py                      # ABC
    rlm_repl.py                 # RLM_REPL class
    repl.py                     # REPLEnv + Sub_RLM
    utils/
      llm.py                    # OpenAIClient
      prompts.py                # System/user prompts
      utils.py                  # find_code_blocks, check_for_final_answer, etc.
    logger/
      root_logger.py
      repl_logger.py

  <span style="color: var(--green);">agents/                      # NEW â€” multi-agent platform</span>
    <span style="color: var(--green);">__init__.py</span>               # exports: Orchestrator, Agent, AgentConfig, ToolRegistry
    <span style="color: var(--green);">types.py</span>                  # AgentConfig, AgentState, AgentEvent, enums (~120 lines)
    <span style="color: var(--green);">events.py</span>                 # EventBus â€” thread-safe pub/sub (~60 lines)
    <span style="color: var(--green);">agent.py</span>                  # Agent â€” RLM loop + events (~150 lines)
    <span style="color: var(--green);">orchestrator.py</span>           # Orchestrator â€” thread pool + state (~80 lines)
    <span style="color: var(--green);">tools.py</span>                  # ToolRegistry + mock tools (~100 lines)
    <span style="color: var(--green);">prompts.py</span>                # Agent-type prompt addenda (~40 lines)

  <span style="color: var(--green);">server.py</span>                    # NEW â€” FastAPI REST + SSE (~120 lines)
  <span style="color: var(--green);">ui.html</span>                      # NEW â€” Dashboard single file (~400 lines)
  <span style="color: var(--green);">run_platform.py</span>              # NEW â€” Entry point (~15 lines)

  main.py                       # EXISTING â€” needle-in-haystack example
  test_simple.py                # EXISTING â€” simple test
  test_dataset_curation.py      # EXISTING â€” dataset test
  requirements.txt              # ADD: fastapi, uvicorn[standard]</code></pre>

<p><strong>Total new code: ~1100 lines</strong> across 10 files. Existing <code>rlm/</code> package is completely untouched.</p>
</section>

<!-- ================================================================ -->
<section id="implementation">
<h2>8. Implementation Steps</h2>

<div class="step-card">
  <h4><span class="step-num">1</span> <code>agents/types.py</code> â€” Data types</h4>
  <p>All enums, dataclasses, and serialization. Foundation for everything else. No external dependencies.</p>
</div>

<div class="step-card">
  <h4><span class="step-num">2</span> <code>agents/events.py</code> â€” EventBus</h4>
  <p>Thread-safe pub/sub with <code>threading.Lock</code>, per-subscriber <code>queue.Queue</code>, history replay for late-joining clients, 30s heartbeat.</p>
</div>

<div class="step-card">
  <h4><span class="step-num">3</span> <code>agents/tools.py</code> â€” ToolRegistry + mock tools</h4>
  <p>Registry: <code>register(name, func, desc)</code>, <code>get_tools(names)</code>, <code>get_tools_prompt_section(names)</code>. Mock tools: <code>generate_image</code> (returns fake path), <code>save_json</code>, <code>load_json</code>, <code>analyze_dataset</code>, <code>augment_dataset</code>.</p>
</div>

<div class="step-card">
  <h4><span class="step-num">4</span> <code>agents/prompts.py</code> â€” Agent-type prompt addenda</h4>
  <p>Short text blocks appended to the RLM system prompt: tool_chain instructions ("use tools directly, 1-3 iterations"), hybrid instructions ("combine llm_query + tools").</p>
</div>

<div class="step-card">
  <h4><span class="step-num">5</span> <code>agents/agent.py</code> â€” Agent class (core)</h4>
  <p>The main class. Re-implements the <code>RLM_REPL.completion()</code> loop calling the same functions from <code>rlm.utils.utils</code>. Emits events via EventBus at each step. ~150 lines.</p>
  <p><strong>Depends on:</strong> types.py, events.py, tools.py, prompts.py + <code>rlm.utils.utils</code>, <code>rlm.utils.prompts</code>, <code>rlm.repl.REPLEnv</code>, <code>rlm.utils.llm.OpenAIClient</code></p>
</div>

<div class="step-card">
  <h4><span class="step-num">6</span> <code>agents/orchestrator.py</code> â€” Orchestrator</h4>
  <p>Owns EventBus + ToolRegistry + ThreadPoolExecutor. Provides <code>launch_agent()</code>, <code>get_all_states()</code>, <code>cancel_agent()</code>. ~80 lines.</p>
</div>

<div class="step-card">
  <h4><span class="step-num">7</span> <code>agents/__init__.py</code> â€” Exports</h4>
  <p>Clean public API: <code>from agents import Orchestrator, Agent, AgentConfig, ToolRegistry</code></p>
</div>

<div class="step-card">
  <h4><span class="step-num">8</span> <code>server.py</code> â€” FastAPI server</h4>
  <p>7 endpoints (see table above). The SSE endpoint bridges blocking EventBus to async via a reader thread + <code>asyncio.Queue</code>. ~120 lines.</p>
</div>

<div class="step-card">
  <h4><span class="step-num">9</span> <code>ui.html</code> â€” Web dashboard</h4>
  <p>Single HTML file, vanilla JS + CSS. Dark theme, agent cards with real-time updates via <code>EventSource</code>. Launch form, progress bars, code display, stdout/stderr, final answers. ~400 lines.</p>
</div>

<div class="step-card">
  <h4><span class="step-num">10</span> <code>run_platform.py</code> â€” Entry point + deps</h4>
  <p>Tiny script: argparse for --port, runs uvicorn. Add <code>fastapi</code> and <code>uvicorn[standard]</code> to requirements.txt.</p>
</div>
</section>

<!-- ================================================================ -->
<section id="ui-design">
<h2>9. UI Design</h2>

<h3>Layout</h3>
<pre><code>+----------------------------------------------------------+
|  RLM Multi-Agent Platform                    [Launch Form] |
|  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” |
|  â”‚ Agent Name: [______] â”‚ â”‚ Context (optional):         â”‚ |
|  â”‚ Task: [____________] â”‚ â”‚ [________________________]  â”‚ |
|  â”‚ Type: [recursive â–¾]  â”‚ â”‚                             â”‚ |
|  â”‚         [Launch]     â”‚ â”‚                             â”‚ |
|  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ |
+----------------------------------------------------------+
|                                                            |
|  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         |
|  â”‚ Legal Analyst    ğŸŸ¢  â”‚ â”‚ Image Creator   ğŸ”µ  â”‚         |
|  â”‚ recursive_rlm       â”‚ â”‚ tool_chain          â”‚         |
|  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 6/20     â”‚ â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 2/3      â”‚         |
|  â”‚                     â”‚ â”‚                     â”‚         |
|  â”‚ â”Œâ”€ Code â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€ Code â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         |
|  â”‚ â”‚ sections = ...  â”‚ â”‚ â”‚ â”‚ img = generate_ â”‚ â”‚         |
|  â”‚ â”‚ for s in sect:  â”‚ â”‚ â”‚ â”‚   image("cat")  â”‚ â”‚         |
|  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         |
|  â”‚ stdout: Found 12   â”‚ â”‚ stdout: Saved to   â”‚         |
|  â”‚   relevant clauses â”‚ â”‚   /tmp/cat.png     â”‚         |
|  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         |
|                                                            |
|  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  |
|  â”‚ Dataset Curator  ğŸŸ¢  â”‚                                  |
|  â”‚ hybrid               â”‚                                  |
|  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ 4/10     â”‚                                  |
|  â”‚                     â”‚                                  |
|  â”‚ â”Œâ”€ Code â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                                  |
|  â”‚ â”‚ fixed = llm_que â”‚ â”‚                                  |
|  â”‚ â”‚  ry("Fix: ...")  â”‚ â”‚                                  |
|  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                                  |
|  â”‚ stdout: Fixed 3    â”‚                                  |
|  â”‚   typos, filled 2  â”‚                                  |
|  â”‚   missing labels   â”‚                                  |
|  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  |
+----------------------------------------------------------+</code></pre>

<h3>Card States</h3>
<table>
  <thead><tr><th>Status</th><th>Visual</th></tr></thead>
  <tbody>
    <tr><td>pending</td><td>Gray border, dimmed</td></tr>
    <tr><td>running</td><td>Blue left border, animated progress bar, code updates live</td></tr>
    <tr><td>completed</td><td>Green left border, green badge, final answer section visible</td></tr>
    <tr><td>failed</td><td>Red left border, red badge, error message shown</td></tr>
    <tr><td>cancelled</td><td>Orange left border, orange badge</td></tr>
  </tbody>
</table>

<h3>Tech Stack</h3>
<ul>
  <li><strong>Single HTML file</strong> â€” no build tools, no npm, no React</li>
  <li><strong>Vanilla JS</strong> â€” EventSource API for SSE, DOM manipulation</li>
  <li><strong>CSS Grid</strong> â€” responsive card layout</li>
  <li><strong>Monospace font</strong> â€” code-focused aesthetic</li>
</ul>
</section>

<!-- ================================================================ -->
<section id="tools">
<h2>10. Tool System</h2>

<h3>How Tools Work</h3>
<p>Tools are plain Python functions registered in a <code>ToolRegistry</code>. When an agent starts, its configured tools are injected into <code>repl_env.globals</code>, making them callable from LLM-generated code.</p>

<pre><code># Registration
registry = ToolRegistry()
registry.register("generate_image", my_func, "Generate an AI image from prompt")

# Agent config
config = AgentConfig(
    name="Image Creator",
    task="Create a cat image",
    agent_type=AgentType.TOOL_CHAIN,
    tools=["generate_image", "save_json"],
)

# Inside Agent.run():
repl_env.globals["generate_image"] = registry.get("generate_image")
repl_env.globals["save_json"] = registry.get("save_json")

# Now the LLM can write:
# ```repl
# path = generate_image("a fluffy orange cat", "cat.png")
# print(f"Image saved to {path}")
# ```</code></pre>

<h3>Mock Tools (for testing without extra API keys)</h3>
<table>
  <thead><tr><th>Tool Name</th><th>Mock Behavior</th><th>Real Behavior (later)</th></tr></thead>
  <tbody>
    <tr>
      <td><code>generate_image(prompt, path)</code></td>
      <td>Returns <code>"/tmp/mock_image.png"</code>, creates a small placeholder file</td>
      <td>Calls DALL-E 3 API, saves actual image</td>
    </tr>
    <tr>
      <td><code>make_3d(image_path)</code></td>
      <td>Returns <code>"/tmp/mock_model.obj"</code></td>
      <td>Calls a 3D generation API</td>
    </tr>
    <tr>
      <td><code>save_json(data, path)</code></td>
      <td>Actually saves JSON (real, cheap)</td>
      <td>Same</td>
    </tr>
    <tr>
      <td><code>load_json(path)</code></td>
      <td>Actually loads JSON (real, cheap)</td>
      <td>Same</td>
    </tr>
    <tr>
      <td><code>analyze_dataset(data)</code></td>
      <td>Returns basic stats dict (row count, null count, column types)</td>
      <td>Same + more sophisticated profiling</td>
    </tr>
    <tr>
      <td><code>augment_dataset(data, n, llm_func)</code></td>
      <td>Returns data unchanged with a note</td>
      <td>Calls llm_func to generate synthetic rows</td>
    </tr>
  </tbody>
</table>

<h3>Prompt Integration</h3>
<p>The <code>ToolRegistry</code> generates a prompt section describing available tools, which is appended to the system prompt:</p>
<pre><code>Available tool functions in your REPL environment:
- generate_image(prompt: str, output_path: str = "output.png") -> str:
    Generate an image from a text prompt. Returns file path.
- save_json(data: Any, path: str = "output.json") -> str:
    Save data as JSON file.
- make_3d(image_path: str, output_path: str = "model.obj") -> str:
    Convert a 2D image to a 3D model. Returns file path.</code></pre>
</section>

<!-- ================================================================ -->
<section id="concurrency">
<h2>11. Concurrency &amp; Safety</h2>

<h3>Why Threading Works Here</h3>
<div class="callout-green callout">
  <strong>Agents are I/O-bound, not CPU-bound.</strong> Each agent spends most of its time waiting for OpenAI API responses (seconds per call). The Python GIL doesn't matter because threads release it during I/O. A <code>ThreadPoolExecutor(max_workers=8)</code> can handle 8+ agents comfortably.
</div>

<h3>Isolation Guarantees</h3>
<table>
  <thead><tr><th>Resource</th><th>Shared?</th><th>Thread-Safe?</th></tr></thead>
  <tbody>
    <tr><td>OpenAIClient</td><td>No â€” each agent creates its own</td><td>N/A</td></tr>
    <tr><td>REPLEnv</td><td>No â€” each agent creates its own</td><td>Has <code>self._lock</code> for exec()</td></tr>
    <tr><td>REPLEnv.globals</td><td>No â€” isolated per agent</td><td>N/A</td></tr>
    <tr><td>REPLEnv.locals</td><td>No â€” isolated per agent</td><td>N/A</td></tr>
    <tr><td>Temp directory</td><td>No â€” unique per REPLEnv</td><td>N/A</td></tr>
    <tr><td>EventBus</td><td>Yes â€” shared by all agents</td><td>Yes (<code>threading.Lock</code>)</td></tr>
    <tr><td>Orchestrator._agents dict</td><td>Yes â€” shared</td><td>Yes (<code>threading.Lock</code>)</td></tr>
  </tbody>
</table>

<h3>Cancellation</h3>
<p>Cooperative cancellation: <code>agent.cancel()</code> sets status to CANCELLED. The agent checks this at the top of each iteration loop. True mid-execution cancellation (interrupting a running API call) isn't supported &mdash; Python threads can't be interrupted cleanly. The cooperative approach is sufficient since each iteration is bounded.</p>
</section>

<!-- ================================================================ -->
<section id="decisions">
<h2>12. Design Decisions &amp; Trade-offs</h2>

<table>
  <thead><tr><th>Decision</th><th>Why</th><th>Trade-off</th></tr></thead>
  <tbody>
    <tr>
      <td><strong>Re-implement RLM loop in Agent</strong> instead of subclassing RLM_REPL</td>
      <td>Zero modifications to <code>rlm/</code> package. Full event emission control.</td>
      <td>If <code>rlm_repl.py</code> changes, Agent loop needs updating. But the loop is only ~60 lines calling stable utility functions.</td>
    </tr>
    <tr>
      <td><strong>Threading</strong> over asyncio</td>
      <td><code>exec()</code>, OpenAI sync client, file I/O &mdash; all synchronous. Threading is natural for I/O-bound work.</td>
      <td>GIL limits CPU parallelism, but agents are I/O-bound so this doesn't matter.</td>
    </tr>
    <tr>
      <td><strong>SSE</strong> over WebSocket</td>
      <td>One-way serverâ†’client push is all we need for events. SSE auto-reconnects, no special client library needed.</td>
      <td>Can't send messages back through the stream (but we use REST POST for that).</td>
    </tr>
    <tr>
      <td><strong>Single HTML file</strong> over React/Svelte</td>
      <td>Zero build tooling. Drop-in file. The full <code>rlm</code> repo already has a React visualizer &mdash; we deliberately keep this simple.</td>
      <td>Limited UI sophistication. Fine for a monitoring dashboard.</td>
    </tr>
    <tr>
      <td><strong>Mock tools first</strong></td>
      <td>Test the full pipeline (orchestrator â†’ agent â†’ tools â†’ events â†’ UI) without extra API costs or keys.</td>
      <td>Need to swap in real implementations later. But the interface is identical.</td>
    </tr>
    <tr>
      <td><strong>EventBus with history replay</strong></td>
      <td>Late-joining UI clients (page refresh, new tab) immediately see all current agent states without a separate state-sync mechanism.</td>
      <td>Memory grows with events. Capped at 1000.</td>
    </tr>
    <tr>
      <td><strong>OpenAI-only backend</strong></td>
      <td>Matches rlm-minimal. Simpler to start. Can add backends by swapping OpenAIClient.</td>
      <td>Locked to OpenAI for now. But the Agent just calls <code>OpenAIClient.completion()</code> &mdash; easy to abstract later.</td>
    </tr>
  </tbody>
</table>
</section>

<!-- ================================================================ -->
<section id="future">
<h2>13. Future Extensions</h2>

<h3>Near-term (once core works)</h3>
<ul>
  <li><strong>Real tools:</strong> Swap mock generate_image for DALL-E 3 API, add real HuggingFace dataset loading</li>
  <li><strong>Agent-to-agent communication:</strong> Agent A's output becomes Agent B's context. Add a <code>pipe(agent_a, agent_b)</code> primitive.</li>
  <li><strong>Persistent agents:</strong> Use RLM's persistent mode for multi-turn agent conversations</li>
  <li><strong>Multiple LLM backends:</strong> Drop-in Anthropic/Gemini clients alongside OpenAI</li>
</ul>

<h3>Medium-term</h3>
<ul>
  <li><strong>Agent templates:</strong> Pre-configured agent types ("Legal Analyst", "Code Reviewer", "Data Scientist") with appropriate tools and prompts</li>
  <li><strong>Workflow DAGs:</strong> Define multi-step workflows where agents feed into each other</li>
  <li><strong>Cost tracking:</strong> Token usage per agent, budget limits</li>
  <li><strong>Docker environments:</strong> Use the full RLM's DockerREPL for sandboxed execution</li>
</ul>

<h3>Long-term</h3>
<ul>
  <li><strong>Distributed execution:</strong> Agents on different machines, event streaming over network</li>
  <li><strong>Persistent state:</strong> Save/resume agent sessions across server restarts</li>
  <li><strong>Plugin ecosystem:</strong> Third-party tool packages</li>
</ul>
</section>

<footer>
  RLM Multi-Agent Platform Architecture Plan &mdash; Built on <code>rlm-minimal/</code>
</footer>

</body>
</html>
